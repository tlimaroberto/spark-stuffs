{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "grpby_agg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO+ZLbCHqpz+akaArxPXaPd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlimaroberto/spark-stuffs/blob/main/grpby_agg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "LCabCyryHGwk",
        "outputId": "8c124e13-6332-42c4-b8f7-4d7d4d1b8a7b"
      },
      "source": [
        "!apt update\n",
        "!apt install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"]= \"/usr/lib/jvm/java-8-openjdk-amd64/\"\n",
        "os.environ[\"SPARK_HOME\"]= \"/content/spark-2.3.1-bin-hadoop2.7/\"\n",
        "\n",
        "!ls\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\u001b[0m\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rHit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Conn\u001b[0m\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Conn\u001b[0m\r                                                                               \rGet:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Fetched 252 kB in 2s (132 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "39 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "sample_data\t\t\t spark-2.3.1-bin-hadoop2.7.tgz.2\n",
            "spark-2.3.1-bin-hadoop2.7\t spark-2.3.1-bin-hadoop2.7.tgz.3\n",
            "spark-2.3.1-bin-hadoop2.7.tgz\t spark-2.3.1-bin-hadoop2.7.tgz.4\n",
            "spark-2.3.1-bin-hadoop2.7.tgz.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://1034f19a2608:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f7bbb2ce1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqxbasSLP2nX"
      },
      "source": [
        "df = spark.read.csv(\"challenge.csv\", header = True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yokYUmYoQtZ0",
        "outputId": "5f4ae9dd-9b68-4992-dd5e-1fad030d3821"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+--------------+-----------------+----------+\n",
            "|     ip_address|       Country|      Domain Name|Bytes_used|\n",
            "+---------------+--------------+-----------------+----------+\n",
            "|  52.81.192.172|         China| odnoklassniki.ru|       463|\n",
            "| 119.239.207.13|         China|         youtu.be|        51|\n",
            "|  68.69.217.210|         China|        adobe.com|        10|\n",
            "|   7.191.21.223|      Bulgaria|     linkedin.com|       853|\n",
            "|   211.13.10.68|     Indonesia|          hud.gov|        29|\n",
            "|   239.80.21.97|      Suriname|       smh.com.au|       218|\n",
            "|106.214.106.233|       Jamaica|    amazonaws.com|        95|\n",
            "| 127.242.24.138|         China| surveymonkey.com|       123|\n",
            "|     99.2.6.139|Czech Republic|     geocities.jp|       322|\n",
            "|   237.54.11.63|         China|       amazon.com|        83|\n",
            "| 252.141.157.25|         Japan|      cornell.edu|       374|\n",
            "|185.220.128.248|       Belgium|       weebly.com|       389|\n",
            "|   151.77.19.45|   Afghanistan|independent.co.uk|       282|\n",
            "|  9.161.158.225|     Indonesia|    bloglines.com|       726|\n",
            "| 156.144.61.155|Czech Republic|   slideshare.net|       657|\n",
            "|   8.96.188.151|     Indonesia|          ibm.com|       517|\n",
            "|      5.72.7.65|        Mexico|         youtu.be|       877|\n",
            "|227.110.112.144|       Croatia|         ehow.com|       287|\n",
            "|    81.71.28.97|      Thailand|          last.fm|       588|\n",
            "|  9.255.129.184|      Thailand|          mtv.com|       114|\n",
            "+---------------+--------------+-----------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLn2QKRvRXlk"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "df = df.withColumn('is_Mexico', when(df.Country == 'Mexico','yes').otherwise('no'))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAFE8lerRi5m",
        "outputId": "deb9a13c-9162-4666-f5f0-eebca43fdf90"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+--------------+-----------------+----------+--------+---------+\n",
            "|     ip_address|       Country|      Domain Name|Bytes_used|is_China|is_Mexico|\n",
            "+---------------+--------------+-----------------+----------+--------+---------+\n",
            "|  52.81.192.172|         China| odnoklassniki.ru|       463|      no|       no|\n",
            "| 119.239.207.13|         China|         youtu.be|        51|      no|       no|\n",
            "|  68.69.217.210|         China|        adobe.com|        10|      no|       no|\n",
            "|   7.191.21.223|      Bulgaria|     linkedin.com|       853|      no|       no|\n",
            "|   211.13.10.68|     Indonesia|          hud.gov|        29|      no|       no|\n",
            "|   239.80.21.97|      Suriname|       smh.com.au|       218|      no|       no|\n",
            "|106.214.106.233|       Jamaica|    amazonaws.com|        95|      no|       no|\n",
            "| 127.242.24.138|         China| surveymonkey.com|       123|      no|       no|\n",
            "|     99.2.6.139|Czech Republic|     geocities.jp|       322|      no|       no|\n",
            "|   237.54.11.63|         China|       amazon.com|        83|      no|       no|\n",
            "| 252.141.157.25|         Japan|      cornell.edu|       374|      no|       no|\n",
            "|185.220.128.248|       Belgium|       weebly.com|       389|      no|       no|\n",
            "|   151.77.19.45|   Afghanistan|independent.co.uk|       282|      no|       no|\n",
            "|  9.161.158.225|     Indonesia|    bloglines.com|       726|      no|       no|\n",
            "| 156.144.61.155|Czech Republic|   slideshare.net|       657|      no|       no|\n",
            "|   8.96.188.151|     Indonesia|          ibm.com|       517|      no|       no|\n",
            "|      5.72.7.65|        Mexico|         youtu.be|       877|     yes|      yes|\n",
            "|227.110.112.144|       Croatia|         ehow.com|       287|      no|       no|\n",
            "|    81.71.28.97|      Thailand|          last.fm|       588|      no|       no|\n",
            "|  9.255.129.184|      Thailand|          mtv.com|       114|      no|       no|\n",
            "+---------------+--------------+-----------------+----------+--------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "bpPlyeh5TbE4",
        "outputId": "8a16f8ce-adfa-46b6-bd0a-6b987e925cd8"
      },
      "source": [
        "import pyspark.sql.functions as sqlfunct\n",
        "df1 = df.groupby('is_Mexico').agg(sqlfunc.sum(df.Bytes_used).alias('Soma'))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-a26660a446d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msqlfunct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'is_Mexico'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytes_used\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Soma'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Soma'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1182\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1183\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'desc'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR5qrjdqUB_e",
        "outputId": "af3b05ec-352b-400c-b726-b0f844e838e3"
      },
      "source": [
        "df1.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+--------+\n",
            "|is_Mexico|    Soma|\n",
            "+---------+--------+\n",
            "|       no|508076.0|\n",
            "|      yes|  6293.0|\n",
            "+---------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF69cRfwUbsP",
        "outputId": "cf170427-bfb5-46f1-fb8f-dbeafd7afd52"
      },
      "source": [
        "df2 = df.groupby(\"Country\").agg(sqlfunc.countDistinct(df.ip_address).alias('Total'))\n",
        "df2.sort(col('Total').desc()).show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+-----+\n",
            "|       Country|Total|\n",
            "+--------------+-----+\n",
            "|         China|  172|\n",
            "|     Indonesia|  114|\n",
            "|   Philippines|   65|\n",
            "|        Russia|   56|\n",
            "|        Brazil|   35|\n",
            "|        Poland|   31|\n",
            "|        Sweden|   28|\n",
            "|         Japan|   25|\n",
            "|      Portugal|   23|\n",
            "|Czech Republic|   23|\n",
            "|        France|   21|\n",
            "|          Peru|   19|\n",
            "|      Colombia|   17|\n",
            "| United States|   15|\n",
            "|     Argentina|   14|\n",
            "|       Ukraine|   14|\n",
            "|        Mexico|   13|\n",
            "|      Thailand|   12|\n",
            "|       Nigeria|   11|\n",
            "|        Canada|   11|\n",
            "+--------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}